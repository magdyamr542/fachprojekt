{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2443ab094d5b166a05dc64e1f68f4afe",
     "grade": false,
     "grade_id": "cell-167675742f4ab308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**Fachprojekt Dokumentenanalyse** *WS 22/23* -- *Philipp Oberdiek, Gernot A. Fink* -- *Technische Universität Dortmund, Lehrstuhl XII, Mustererkennung in eingebetteten Systemen*\n",
    "---\n",
    "# Aufgabe 3: Kreuzvalidierung, Term Weighting\n",
    "\n",
    "\n",
    "Um eine willkürliche Aufteilung der Daten in Training und Test zu vermeiden (machen Sie sich bewusst warum das problematisch ist), verwendet man zur Evaluierung von Klassifikatoren eine Kreuzvalidierung. Dabei wird der gesamte Datensatz in k disjunkte Ausschnitte (Folds) aufgeteilt. Jeder dieser Ausschnitte wird einmal als Test Datensatz verwendet, während alle anderen k-1 Ausschnitte als Trainings Datensatz verwendet werden. Man erhählt also k Gesamtfehlerraten und k klassenspezifische Fehlerraten die man jeweils zu einer gemeinsamen Fehlerrate für die gesamte Kreuzvalidierung mittelt. Beachten Sie, dass dabei ein gewichtetes Mittel gebildet werden muss, da die einzelnen Test Folds nicht unbedingt gleich groß sein müssen.\n",
    "\n",
    "Erweitern Sie ihre Implementierung des Nächste-Nachbar Klassifikators auf mehrere Nachbarn. Führen Sie aufbauend auf den Ergebnissen aus Aufgabe 2 eine 5-Fold Kreuzvalidierung für den k-Nächste-Nachbarn Klassifikator auf dem Brown Corpus durch. Dazu können Sie die Klasse `CrossValidation` im [`evaluation`](../common/evaluation.py) Modul verwenden.\n",
    "\n",
    "Vollziehen Sie dazu nach wie die Klasse die Daten in Training und Test Folds aufteilt. Fertigen Sie zu dem Schema eine Skizze an. Diskutieren Sie Vorteile und Nachteile. Schauen Sie sich an, wie die eigentliche Kreuzvalidierung funktioniert.\n",
    "\n",
    "**Hinweise:**\n",
    "\n",
    "Die Klasse `CrossValidator` verwendet die Klasse `ClassificationEvaluator`, die Sie schon für Aufgabe 2 implementieren sollten. Kontrollieren Sie Ihre Umsetzung im Sinne der Verwendung im `CrossValidator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15a410736b46163169ff91fa5e893a5f",
     "grade": false,
     "grade_id": "cell-40f404bc4da71f90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "\n",
    "from common.corpus import CorpusLoader\n",
    "brown = CorpusLoader.brown_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed3b8f831f4ab706f2097283d33f1064",
     "grade": false,
     "grade_id": "cell-52e456ac5ee4fce3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Testen Sie ihre Implementierung des erweiterten Nächste-Nachbar Klassifikators mit folgendem Unittest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aec38845bdbf43a2209e787e98133636",
     "grade": false,
     "grade_id": "cell-fa28f0e5ec358438",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "from utest.test_classification import ClassificationTest\n",
    "\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(ClassificationTest(\"test_knn\"))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from common.features import BagOfWords, WordListNormalizer\n",
    "from common.classification import KNNClassifier\n",
    "from common.evaluation import CrossValidation\n",
    "\n",
    "# Führen Sie hier die 5-fache Kreuzvalidierung durch\n",
    "normalizer = WordListNormalizer()\n",
    "_, stemmed_words = normalizer.normalize_words(brown.words())\n",
    "vocabulary = BagOfWords.most_freq_words(stemmed_words, n_words=500)\n",
    "\n",
    "category_wordlists_dict = {}\n",
    "for cat in brown.categories():\n",
    "    documents = brown.fileids(categories=cat)\n",
    "    category_wordlists_dict[cat] = [brown.words(fileids=doc) for doc in documents]\n",
    "\n",
    "bow = BagOfWords(vocabulary=vocabulary)\n",
    "category_bow_dict = bow.category_bow_dict(category_wordlists_dict)\n",
    "\n",
    "knn_classifier = KNNClassifier(k_neighbors=3, metric='euclidean')\n",
    "\n",
    "cv = CrossValidation(category_bow_dict=category_bow_dict, n_folds=5)\n",
    "error_rates = cv.validate(classifier=knn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification error rate: error_rate')\n",
    "print('\\t{:15}: {:6,.2f} %'.format('Overall', error_rates[0]))\n",
    "\n",
    "print('Class specific :')\n",
    "for category, err in error_rates[1]:\n",
    "    print('\\t{:15}: {:6,.2f} %'.format(category, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Weighting\n",
    "\n",
    "Bisher enthalten die Bag-of-Words Histogramme absolute Frequenzen. Dadurch sind die Repräsentationen abhängig von der absoluten Anzahl von Wörtern in den Dokumenten. Dies kann vermieden werden, indem man die Bag-of-Words Histogramme mit einem Normalisierungsfaktor gewichtet.\n",
    "\n",
    "Normalisieren Sie die Bag-of-Words Histogramme so, dass relative Frequenzen verwendet werden. Implementieren und verwenden Sie die Klasse `RelativeTermFrequencies` im [`features`](../common/features.py) Modul.\n",
    "\n",
    "Wie erklären Sie das Ergebnis? Schauen Sie sich dazu noch einmal die mittlere Anzahl von Wörtern pro Dokument an (Aufgabe 2).\n",
    "\n",
    "Wie in der Literatur üblich, verwenden wir den Begriff des \"Term\". Ein Term bezeichnet ein Wort aus dem Vokabular über dem die Bag-of-Words Histogramme gebildet werden. Ein Bag-of-Words Histogramm wird daher auch als Term-Vektor bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from common.features import RelativeTermFrequencies\n",
    "from common.visualization import bar_plot\n",
    "\n",
    "bow = BagOfWords(vocabulary, term_weighting=RelativeTermFrequencies())\n",
    "category_bow_dict = bow.category_bow_dict(category_wordlists_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in category_bow_dict:\n",
    "    values = category_bow_dict[cat]\n",
    "    bar_plot(None, values.sum(axis=0), title=cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec48baea185ee193d739c9dac0a56eb9",
     "grade": false,
     "grade_id": "cell-1986c767af098359",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Zusätzlich kann man noch die inverse Frequenz von Dokumenten berücksichtigen in denen ein bestimmter Term vorkommt. Diese Normalisierung wird als _inverse document frequency_ bezeichnet. Die Idee dahinter ist Wörter, die in vielen Dokumenten vorkommen, weniger stark im Bag-of-Words Histogramm zu gewichten. Die zugrundeliegende Annahme ist ähnlich wie bei den stopwords (Aufgabe 1), dass Wörter, die in vielen Dokumenten vorkommen, weniger Bedeutung für die Unterscheidung von Dokumenten in verschiedene Klassen / Kategorien haben als Wörter, die nur in wenigen Dokumenten vorkommen. Diese Gewichtung lässt sich statistisch aus den Beispieldaten ermitteln.\n",
    "\n",
    "Zusammen mit der relativen Term Gewichtung ergibt sich die so genannte _\"term frequency inverse document frequency\"_.\n",
    "\n",
    "\n",
    "$ \\text{tfidf}(\\text{term},\\text{document}) = \\frac{\\#\\text{term in Dokument}}{ \\#\\text{Wörter in Dokument}} \\times \\log (\\frac{\\#\\text{Dokumente}}{\\#\\text{Dokumente die term enthalten}})$\n",
    "\n",
    "http://www.tfidf.com\n",
    "\n",
    "Erklären Sie die Formel. Plotten Sie die inverse document frequency für jeden Term über dem Brown Corpus.\n",
    "\n",
    "Implementieren und verwenden Sie die Klasse `RelativeInverseDocumentWordFrequencies` im [`features`](../common/features.py) Modul, in der Sie ein tfidf Gewichtungsschema umsetzen. Ermitteln Sie die Gesamt- und klassenspezifischen Fehlerraten mit der Kreuzvalidierung. Vergleichen Sie das Ergebnis mit der absolten und relativen Gewichtung. Erklären Sie die Unterschiede in den klassenspezifischen Fehlerraten. Schauen Sie sich dazu die Verteilungen der Anzahl Wörter und Dokumente je Kategorie aus Aufgabe 1 an. In wie weit ist eine Interpretation möglich?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwort:**\n",
    "\n",
    "Gewichtetes Maß für die Wichtigkeit eines Terms in einem Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from common.visualization import bar_plot\n",
    "from common.features import RelativeInverseDocumentWordFrequencies\n",
    "\n",
    "idf = RelativeInverseDocumentWordFrequencies(vocabulary, category_wordlists_dict)\n",
    "\n",
    "for cat in category_bow_dict:\n",
    "    weighted_values = idf.weighting(category_bow_dict[cat])\n",
    "    bar_plot(None, weighted_values.sum(axis=0), title=cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b4e7d19fb356ce848e8992959e93373",
     "grade": false,
     "grade_id": "cell-51adf05d64784197",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Evaluieren Sie die beste Klassifikationsleistung.\n",
    "\n",
    "Ermitteln Sie nun die Parameter für die beste Klassifikationsleistung des k-nächste-Nachbarn Klassifikators auf dem Brown Corpus mit der Kreuzvalidierung. Dabei wird gleichzeitig immer nur ein Parameter verändert. Man hat eine lokal optimale Parameterkonfiguration gefunden, wenn jede Änderung eines Parameters zu einer Verschlechterung der Fehlerrate führt.\n",
    "\n",
    "Erläutern Sie warum eine solche Parameterkonfiguration lokal optimal ist.\n",
    "\n",
    "Testen Sie mindestens die angegebenen Werte für die folgenden Parameter:\n",
    "1. Größe des Vokabulars typischer Wörter (100, 500, 1000, 2000)\n",
    "2. Gewichtung der Bag-of-Words Histogramme (absolute, relative, relative with inverse document frequency)\n",
    "3. Distanzfunktion für die Bestimmung der nächsten Nachbarn (Cityblock, Euclidean, Cosine)\n",
    "4. Anzahl der betrachteten nächsten Nachbarn (1, 2, 3, 4, 5, 6)\n",
    "\n",
    "Erklären Sie den Effekt aller Parameter.\n",
    "\n",
    "Erklären Sie den Effekt zwischen Gewichtungsschema und Distanzfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.features import AbsoluteTermFrequencies, RelativeTermFrequencies, RelativeInverseDocumentWordFrequencies\n",
    "from itertools import product\n",
    "\n",
    "# prepare combinations\n",
    "n_words = [100, 500, 1000, 2000]\n",
    "weighting = [AbsoluteTermFrequencies, RelativeTermFrequencies, RelativeInverseDocumentWordFrequencies]\n",
    "distances = ['cityblock', 'euclidean', 'cosine']\n",
    "max_neighbors = 6\n",
    "k_neighbors = list(range(1, max_neighbors + 1))\n",
    "combinations = list(product(k_neighbors, distances))\n",
    "\n",
    "# [((params), (results))]\n",
    "results = []\n",
    "\n",
    "# setup data\n",
    "normalizer = WordListNormalizer()\n",
    "_, stemmed_words = normalizer.normalize_words(brown.words())\n",
    "category_wordlists_dict = {}\n",
    "\n",
    "for cat in brown.categories():\n",
    "    documents = brown.fileids(categories=cat)\n",
    "    category_wordlists_dict[cat] = [\n",
    "        brown.words(fileids=doc) for doc in documents]\n",
    "\n",
    "# test all combinations\n",
    "for n_word in n_words:\n",
    "    vocabulary = BagOfWords.most_freq_words(stemmed_words, n_words=n_word)\n",
    "\n",
    "    for weight_measure in weighting:\n",
    "        bow = BagOfWords(vocabulary=vocabulary, term_weighting=weight_measure(vocabulary, category_wordlists_dict))\n",
    "        category_bow_dict = bow.category_bow_dict(category_wordlists_dict)\n",
    "\n",
    "        for k_neighbor, distance_metric in combinations:\n",
    "            print(f\"Evaluating... {n_word}, {str(weight_measure)}, {k_neighbor}, {distance_metric}\")\n",
    "\n",
    "            knn = KNNClassifier(k_neighbors=k_neighbor, metric=distance_metric)\n",
    "            cv = CrossValidation(category_bow_dict, 5)\n",
    "            error_rates = cv.validate(knn)\n",
    "\n",
    "            settings = (n_word, str(weight_measure), k_neighbor, distance_metric)\n",
    "            results.append((settings, error_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: x[1][0] == min(\n",
    "    list(map(lambda x: x[1][0], results))), results))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('fachprojekt': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "32a5ae791907c7b55ac26730012dcd40acf73ce546d19557a180c71163dc3b49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
